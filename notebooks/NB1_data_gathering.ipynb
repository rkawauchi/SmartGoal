{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**SUMARRY:** Using complex Python packages, more than 10,000 data was extracted from the Indiegogo.com website. The code also cleans and formats so that the data is readily available and easy to transfer.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Gathering from Indiegogo.com"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From indiegogo website, we can find pages that have a list of cells for different campaigns. These cells have only part of the information about the campaign. Therefore, if we want to know more details about the project, we have to go into its individual page. After clicking on one cell, we will be directed into the campaign page that has all the informaiton we want about the project. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this in mind, out data scraping code will be divided into two parts. The first part would be mainly responsible for scraping a \n",
      "list of URLs of individual campaign pages. The second part would be in charege of scraping the specific information we want from each\n",
      "campaign page by using the url for each campaign."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "First Part - A list of URLs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import json\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "indiegogo_baseurl = 'http://www.indiegogo.com'\n",
      "\n",
      "# We would like to know if there were actually 13,000 campaigns that we can scrape the information from\n",
      "# What we do here is we use \"status_code\" attribute of a requests.get() object to determine if \n",
      "# infiegogo website actually has the amount of data we set (In our case, we set the number as 13,000.)\n",
      "\n",
      "count_pgnum = 0\n",
      "for pgnum in range(1, 13000):\n",
      "    proj_list_page_url = indiegogo_baseurl + '/explore?filter_browse_balance=true&filter_category=&filter_city=&filter_country=CTRY_US&filter_funding=&filter_percent_funded=&filter_quick=popular_all&filter_status=&filter_title=&per_page={0}'.format(pgnum)    \n",
      "\n",
      "    response = requests.get(proj_list_page_url)\n",
      "    \n",
      "    if response.status_code == 200:\n",
      "        del response\n",
      "    else: \n",
      "        count_pgnum = pgnum - 1\n",
      "        break \n",
      "\n",
      "# Now, count_pgnum is set to a certain number that will be used as pgnum for scraping a list of URLs \n",
      "\n",
      "project_links = [] # global list of links\n",
      "\n",
      "pgnum = count_pgnum\n",
      "\n",
      "# construct url\n",
      "project_list_page_url = indiegogo_baseurl + '/explore?filter_browse_balance=true&filter_category=&filter_city=&filter_country=CTRY_US&filter_funding=&filter_percent_funded=&filter_quick=popular_all&filter_status=&filter_title=&per_page={0}'.format(pgnum)\n",
      "\n",
      "# send GET request to the website server (Indiegogo.com)\n",
      "response = requests.get(project_list_page_url)\n",
      "\n",
      "# check if request was successful\n",
      "if response.status_code != 200:\n",
      "    print 'There was an error!'\n",
      "\n",
      "# retrieve HTML string from content of response\n",
      "html = response.content\n",
      "\n",
      "# construct soup object\n",
      "soup = BeautifulSoup(html)\n",
      "\n",
      "# The url of the individual campaign is stored in <a> tag for example: <a href=\"/projects/solar-roadways\" class=\"i-project\">\n",
      "# Since the information we want is the value of href, we search the index with 'href' to get the URL of the project page \n",
      "for i in soup.findAll('a', {'class': 'i-project'}):\n",
      "    link_suffix = i['href']\n",
      "    full_link = indiegogo_baseurl + link_suffix # construct the full link\n",
      "    project_links.append(full_link) # add full link to our global list\n",
      "        \n",
      "# save links to a file so we don't have to run this code again\n",
      "json.dump(project_links, open('links.json', 'w'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unfortunately, Indiegogo had changed its website's security level, which means no matter how hard we try, we can only retrieve 120 data at once. So the code we provided above is not working anymore. However, we had already scraped a list of URLs for 13,000 campaigns, which means we have the access to get more details about the project and we are able to move on to data analysis."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Second Part - Scrape Information from Each Campaign"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import all of the modules we need  \n",
      "import requests \n",
      "import json\n",
      "import codecs\n",
      "from datetime import datetime\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def parse_page(link):\n",
      "    data_dict = {}\n",
      "\n",
      "    response = requests.get(link)\n",
      "    if response.status_code != 200:\n",
      "        print 'Error for link: {0}'.format(link)\n",
      "        return data_dict\n",
      "\n",
      "    html = response.content\n",
      "    soup = BeautifulSoup(html)\n",
      "\n",
      "    # To scrape information about location\n",
      "    span_element = soup.findAll('span', {'class': 'i-keep-it-together'})\n",
      "    if len(span_element) == 2:\n",
      "        location = span_element[0].text.strip()\n",
      "        data_dict['location'] = location # Save the location information into the data dictionary\n",
      "    else:\n",
      "        data_dict['location'] = 'None' # Send the string 'None' to the dictionary if there was no location information\n",
      "\n",
      "    # To scrape information about category\n",
      "    span_element = soup.findAll('span', {'class': 'i-keep-it-together'})\n",
      "    \n",
      "    # The span_element is a list and normally will contain the information of location and category \n",
      "    # However, somehow some projects they don't have any lication information shown\n",
      "    # What we do below is to check the length of the list to make sure the information we get is about the category\n",
      "    \n",
      "    if len(span_element) == 2:\n",
      "        category = span_element[1].text.strip()\n",
      "        data_dict['category'] = category\n",
      "    elif len(span_element) == 1:\n",
      "        category = span_element[0].text.strip()\n",
      "        data_dict['category'] = category\n",
      "\n",
      "    # funding duration\n",
      "    div_element = soup.find('div', {'class': 'i-time-left'})\n",
      "    if div_element.text.strip().split(' ')[0] == '?': # Some projects have '?' sign for how many days left\n",
      "        data_dict = {} \n",
      "        return data_dict\n",
      "\n",
      "    elif int(div_element.text.strip().split(' ')[0]) == 0: # This means 0 days left, which means the project is done its fund raising\n",
      "        paragraph = soup.find('div', {'class': 'i-funding-duration'})\n",
      "        index = 25\n",
      "        dates = paragraph.text[index:]\n",
      "        \n",
      "        start_date = '{0},{1},{2}'.format(dates.split(' ')[0], dates.split(' ')[1], dates.split(' ')[7]) # Get start date as string\n",
      "        end_date = '{0},{1},{2}'.format(dates.split(' ')[5], dates.split(' ')[6][0:2], dates.split(' ')[7]) # Gest end date as string\n",
      "         \n",
      "        before = datetime.strptime(start_date, '%b,%d,%Y') # Change start date to datetime object\n",
      "        after = datetime.strptime(end_date, '%B,%d,%Y') # Change end date to datetime object\n",
      "        c = after - before # Calculate the duration between start date and end date\n",
      "        c.days\n",
      "        data_dict['duration'] = c.days\n",
      "    else: # Incomplete projects\n",
      "        paragraph = soup.find('div', {'class': 'i-funding-duration'})\n",
      "        index = 25\n",
      "        dates = paragraph.text[index:]\n",
      "        \n",
      "        start_date = '{0},{1},{2}'.format(dates.split(' ')[0], dates.split(' ')[1], dates.split(' ')[8])\n",
      "        end_date = '{0},{1},{2}'.format(dates.split(' ')[6], dates.split(' ')[7][0:2], dates.split(' ')[8])\n",
      "         \n",
      "        before = datetime.strptime(start_date, '%b,%d,%Y')\n",
      "        after = datetime.strptime(end_date, '%B,%d,%Y')\n",
      "        c = after - before\n",
      "        c.days\n",
      "        data_dict['duration'] = c.days\n",
      "\n",
      "    # amount raised\n",
      "    span_element = soup.find('span', {'class': 'currency currency-xlarge'})\n",
      "    if len(span_element.text.split(',')) == 1:\n",
      "        money_string = span_element.text[1:]\n",
      "    else:\n",
      "        money_string = span_element.text.split(',')[0][1:]+span_element.text.split(',')[1]\n",
      "   \n",
      "    data_dict['amount_raised'] = money_string\n",
      "    \n",
      "    # goal of raising\n",
      "    span_element = soup.find('div', {'class':'i-raised'})\n",
      "    money_string = span_element.text[10:].split(' ')[0]\n",
      "    data_dict['raising_goal'] = money_string \n",
      "    \n",
      "    # completion level\n",
      "    span_element = soup.find('div', {'class':'i-percent'})\n",
      "    percent_string = span_element.text.strip()[-10:-1]\n",
      "    data_dict['percent'] = percent_string \n",
      "    \n",
      "    return data_dict\n",
      "\n",
      "links = json.load(open('links.json')) # Open links.json that contains all the URLs of projects\n",
      "f = open('indiegogo.csv','w') # Create a file called indiegogo.csv and write\n",
      "f.write(u'location,category,duration,amount_raised,raising_goal,percent\\n') # Write the column names into indiegogo.csb\n",
      "for link in links: # Pick one URL from links.json\n",
      "    data = parse_page(link) # Put the URL taken as an argument of parse_page function\n",
      "    if data:\n",
      "        row = u'{location},{category},{duration},{amount_raised},{raising_goal},{percent}\\n'.format(**data).encode('utf-8','replace')\n",
      "        f.write(row)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Team members responsible for this notebook:\n",
      "\n",
      "* **Po Jui Chiu**: Scrape the data we want from indiegogo website and simultaneously perform a preliminary data cleaning on it"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}